---
---




@inproceedings{singh2023xflt,
  title={XFLT: Exploring Techniques for Generating Cross Lingual Factually Grounded Long Text.},
  preview={ecai.png},
  author={Singh, Bhavyajeet and Hari, Aditya and Mehta, Rahul and Abhishek, Tushar and Gupta, Manish and Varma, Vasudeva},
  booktitle={ECAI},
  pages={2162--2169},
  year={2023},
  selected={true},
  url={https://www.researchgate.net/publication/374326836_XFLT_Exploring_Techniques_for_Generating_Cross_Lingual_Factually_Grounded_Long_Text},
  html={https://www.researchgate.net/publication/374326836_XFLT_Exploring_Techniques_for_Generating_Cross_Lingual_Factually_Grounded_Long_Text},
  abstract={Multiple business scenarios require an automated generation of descriptive human-readable long text from structured input data, where the source is typically a high-resource language and the target is a low or medium resource language. We define the Cross-Lingual Fact to Long Text Generation (XFLT) as a novel natural language generation (NLG) task that involves generating descriptive and human-readable long text in a target language from structured input data (such as fact triples) in a source language. XFLT is challenging because of (a) hallucinatory nature of the state-of-the-art NLG models, (b) lack of good quality training data, and (c) lack of a suitable cross-lingual NLG metric. Unfortunately previous work focuses on different related problem settings (cross-lingual facts to short text or monolingual graph to text) and has made no efforts to handle hallucinations. In this paper, we contribute a novel dataset, XLALIGN with over 64,000 paragraphs across 12 different languages, and English facts. We propose a novel solution to the XFLT task which addresses these challenges by training multilingual Transformer-based encoder-decoder models with coverage prompts and grounded decoding. Further, it improves on the XFLT quality by defining task-specific reward functions and training on them using reinforcement learning. On XLA LIGN , we compare this novel solution with several strong baselines using a new metric, cross-lingual PARENT. We also make our code and data publicly available}
}

@article{10.1371/journal.pone.0287975,
    doi = {10.1371/journal.pone.0287975},
    preview={plos_one.png},
    author = {Singh, Bhavyajeet AND Vaswani, Kunal AND Paruchuri, Sreeharsha AND Saarikallio, Suvi AND Kumaraguru, Ponnurangam AND Alluri, Vinoo},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {“Help! I need some music!”: Analysing music discourse & depression on Reddit},
    year = {2023},
    month = {07},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pone.0287975},
    html = {https://doi.org/10.1371/journal.pone.0287975},
    pages = {1-16},
    selected={true},
    abstract = {Individuals choose varying music listening strategies to fulfill particular mood-regulation goals. However, ineffective musical choices and a lack of cognizance of the effects thereof can be detrimental to their well-being and may lead to adverse outcomes like anxiety or depression. In our study, we use the social media platform Reddit to perform a large-scale analysis to unearth the several music-mediated mood-regulation goals that individuals opt for in the context of depression. A mixed-methods approach involving natural language processing techniques followed by qualitative analysis was performed on all music-related posts to identify the various music-listening strategies and group them into healthy and unhealthy associations. Analysis of the music content (acoustic features and lyrical themes) accompanying healthy and unhealthy associations showed significant differences. Individuals resorting to unhealthy strategies gravitate towards low-valence tracks. Moreover, lyrical themes associated with unhealthy strategies incorporated tracks with low optimism, high blame, and high self-reference. Our findings demonstrate that being mindful of the objectives of using music, the subsequent effects thereof, and aligning both for well-being outcomes is imperative for comprehensive understanding of the effectiveness of music.},
    number = {7},

}


@inproceedings{10.1145/3487553.3524265,
author = {Abhishek, Tushar and Sagare, Shivprasad and Singh, Bhavyajeet and Sharma, Anubhav and Gupta, Manish and Varma, Vasudeva},
title = {XAlign: Cross-lingual Fact-to-Text Alignment and Generation for Low-Resource Languages},
preview={xalign.png},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524265},
html = {https://doi.org/10.1145/3487553.3524265},
doi = {10.1145/3487553.3524265},
abstract = {Multiple critical scenarios need automated generation of descriptive text in low-resource (LR) languages given English fact triples. For example, Wikipedia text generation given English Infoboxes, automated generation of non-English product descriptions using English product attributes, etc. Previous work on fact-to-text (F2T) generation has focused on English only. Building an effective cross-lingual F2T (XF2T) system requires alignment between English structured facts and LR sentences. Either we need to manually obtain such alignment data at a large scale, which is expensive, or build automated models for cross-lingual alignment. To the best of our knowledge, there has been no previous attempt on automated cross-lingual alignment or generation for LR languages. We propose two unsupervised methods for cross-lingual alignment. We contribute XAlign, an XF2T dataset with 0.45M pairs across 8 languages, of which 5402 pairs have been manually annotated. We also train strong baseline XF2T generation models on XAlign. We make our code and dataset publicly available1, and hope that this will help advance further research in this critical area.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {171–175},
numpages = {5},
keywords = {XF2T, deep learning, fact-to-text},
location = {Virtual Event, Lyon, France},
selected={true},
series = {WWW '22}
}




@inproceedings{maheshwari-etal-2021-scibert,
    title = "{S}ci{BERT} Sentence Representation for Citation Context Classification",
    author = "Maheshwari, Himanshu  and
      Singh, Bhavyajeet  and
      Varma, Vasudeva",
    editor = "Beltagy, Iz  and
      Cohan, Arman  and
      Feigenblat, Guy  and
      Freitag, Dayne  and
      Ghosal, Tirthankar  and
      Hall, Keith  and
      Herrmannova, Drahomira  and
      Knoth, Petr  and
      Lo, Kyle  and
      Mayr, Philipp  and
      Patton, Robert M.  and
      Shmueli-Scheuer, Michal  and
      de Waard, Anita  and
      Wang, Kuansan  and
      Wang, Lucy Lu",
    booktitle = "Proceedings of the Second Workshop on Scholarly Document Processing",
    preview={},
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sdp-1.17",
    html = "https://aclanthology.org/2021.sdp-1.17",
    pages = "130--133",
    selected={true},
    abstract = "This paper describes our system (IREL) for 3C-Citation Context Classification shared task of the Scholarly Document Processing Workshop at NAACL 2021. We participated in both subtask A and subtask B. Our best system achieved a Macro F1 score of 0.26973 on the private leaderboard for subtask A and was ranked one. For subtask B our best system achieved a Macro F1 score of 0.59071 on the private leaderboard and was ranked two. We used similar models for both the subtasks with some minor changes, as discussed in this paper. Our best performing model for both the subtask was a finetuned SciBert model followed by a linear layer. This paper provides a detailed description of all the approaches we tried and their results.",
}


@InProceedings{10.1007/978-3-031-70341-6_26,
author="Mehta, Rahul
and Singh, Bhavyajeet
and Varma, Vasudeva
and Gupta, Manish",
preview={cirquit_vqa.png},
editor="Bifet, Albert
and Davis, Jesse
and Krilavi{\v{c}}ius, Tomas
and Kull, Meelis
and Ntoutsi, Eirini
and {\v{Z}}liobait{\.{e}}, Indr{\.{e}}",
title="CircuitVQA: A Visual Question Answering Dataset for Electrical Circuit Images",
booktitle="Machine Learning and Knowledge Discovery in Databases. Research Track",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="440--460",
abstract="A visual question answering (VQA) system for electrical circuit images could be useful as a quiz generator, design and verification assistant or an electrical diagnosis tool. Although there exists a vast literature on VQA, to the best of our knowledge, there is no existing work on VQA for electrical circuit images. To this end, we curate a new dataset, CircuitVQA, of 115K+ questions on 5725 electrical images with {\$}{\$}{\backslash}sim {\$}{\$}∼70 circuit symbols. The dataset contains schematic as well as hand-drawn images. The questions span various categories like counting, value, junction and position based questions. To be effective, models must demonstrate skills like object detection, text recognition, spatial understanding, question intent understanding and answer generation. We experiment with multiple foundational visio-linguistic models for this task and find that a finetuned BLIP model with component descriptions as additional input provides best results. We make the code and dataset publicly available (https://github.com/rahcode7/Circuit-VQA).",
isbn="978-3-031-70341-6",
selected="true",
html="https://link.springer.com/chapter/10.1007/978-3-031-70341-6_26"
}


@inproceedings{singh-etal-2022-massively,
    title = "Massively Multilingual Language Models for Cross Lingual Fact Extraction from Low Resource {I}ndian Languages",
    author = "Singh, Bhavyajeet  and
      Kandru, Siri Venkata Pavan Kumar  and
      Sharma, Anubhav  and
      Varma, Vasudeva",
    editor = "Akhtar, Md. Shad  and
      Chakraborty, Tanmoy",
    booktitle = "Proceedings of the 19th International Conference on Natural Language Processing (ICON)",
    month = dec,
    year = "2022",
    address = "New Delhi, India",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.icon-main.2",
    html = "https://aclanthology.org/2022.icon-main.2",
    pages = "11--18",
    preview={msme.png},
    selected={true},
    abstract = "Massive knowledge graphs like Wikidata attempt to capture world knowledge about multiple entities. Recent approaches concentrate on automatically enriching these KGs from text. However a lot of information present in the form of natural text in low resource languages is often missed out. Cross Lingual Information Extraction aims at extracting factual information in the form of English triples from low resource Indian Language text. Despite its massive potential, progress made on this task is lagging when compared to Monolingual Information Extraction. In this paper, we propose the task of Cross Lingual Fact Extraction(CLFE) from text and devise an end-to-end generative approach for the same which achieves an overall F1 score of 77.46",
}

@inproceedings{sagare-etal-2023-xf2t,
    title = "{XF}2{T}: Cross-lingual Fact-to-Text Generation for Low-Resource Languages",
    preview={xf2t.png},
    author = "Sagare, Shivprasad  and
      Abhishek, Tushar  and
      Singh, Bhavyajeet  and
      Sharma, Anubhav  and
      Gupta, Manish  and
      Varma, Vasudeva",
    editor = "Keet, C. Maria  and
      Lee, Hung-Yi  and
      Zarrie{\ss}, Sina",
    booktitle = "Proceedings of the 16th International Natural Language Generation Conference",
    month = sep,
    year = "2023",
    address = "Prague, Czechia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.inlg-main.2",
    html = "https://aclanthology.org/2023.inlg-main.2",
    doi = "10.18653/v1/2023.inlg-main.2",
    pages = "15--27",
    selected={true},
    abstract = "Multiple business scenarios require an automated generation of descriptive human-readable text from structured input data. This has resulted into substantial work on fact-to-text generation systems recently. Unfortunately, previous work on fact-to-text (F2T) generation has focused primarily on English mainly due to the high availability of relevant datasets. Only recently, the problem of cross-lingual fact-to-text (XF2T) was proposed for generation across multiple languages alongwith a dataset, XAlign for eight languages. However, there has been no rigorous work on the actual XF2T generation problem. We extend XAlign dataset with annotated data for four more languages: Punjabi, Malayalam, Assamese and Oriya. We conduct an extensive study using popular Transformer-based text generation models on our extended multi-lingual dataset, which we call XAlignV2. Further, we investigate the performance of different text generation strategies: multiple variations of pretraining, fact-aware embeddings and structure-aware input encoding. Our extensive experiments show that a multi-lingual mT5 model which uses fact-aware embeddings with structure-aware input encoding leads to best results (30.90 BLEU, 55.12 METEOR and 59.17 chrF++) across the twelve languages. We make our code, dataset and model publicly available, and hope that this will help advance further research in this critical area.",
}








@article{chowdary2024lyrically,
  title={Lyrically Speaking: Exploring the Link Between Lyrical Emotions, Themes and Depression Risk},
  author={Chowdary, Pavani and Singh, Bhavyajeet and Agarwal, Rajat and Alluri, Vinoo},
  journal={arXiv preprint arXiv:2408.15575},
  year={2024},
  preview={lyrically_speaking.png},
  html={http://www.arxiv.org/abs/2408.15575}
}
